{"paragraphs":[{"text":"%pyspark\nimport os\n\nfiles = [\"errors\",\"telemetry\",\"machines\",\"failures\",\"maint\"]\nfor file in files:\n    jsonFilePath = os.environ[\"XAP_HOME\"] + \"/insightedge/data/mini_files/\"+file+\".csv\"\n    mach =spark.read.format(\"csv\").load(jsonFilePath).cache()\n\n    #write data to the Grid\n    mach.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(file)","user":"anonymous","dateUpdated":"2019-06-17T16:15:33+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2019-06-17 16:16:03,642 CONFIG [com.gigaspaces.logger] - Log file: /home/or/Desktop/final_proj/gigaspaces-insightedge-enterprise-14.5.0-m7/logs/2019-06-17~16.16-gigaspaces-service-127.0.1.1-8595.log\n"}]},"apps":[],"jobName":"paragraph_1560763717114_1315507856","id":"20190320-173319_1499293377","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:15:33+0300","dateFinished":"2019-06-17T16:16:14+0300","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1883"},{"text":"%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport os\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import datediff\nfrom pyspark.sql.functions import col, unix_timestamp, round\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.sql.types import DoubleType\n\n# For creating some preliminary EDA plots.\nimport matplotlib.pyplot as plt\n\n\n\n\nspark = SparkSession.builder.getOrCreate()","user":"anonymous","dateUpdated":"2019-06-17T16:16:14+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1560763717119_446845244","id":"20190325-165539_1058509494","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:16:14+0300","dateFinished":"2019-06-17T16:16:15+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1884"},{"text":"%pyspark\n\nmachinesDF = spark.read.format(\"org.apache.spark.sql.insightedge\").option(\"collection\", \"machines\").load().toPandas()\nmachinesDF.columns = ['machineID','model','age']\nmachinesDF = machinesDF[machinesDF.machineID != \"machineID\"]\nprint(machinesDF)\nmachinesDF = sqlContext.createDataFrame(machinesDF)\n\n\n\nerrorsDF = spark.read.format(\"org.apache.spark.sql.insightedge\").option(\"collection\", \"errors\").load().toPandas()\nerrorsDF.columns = ['datetime','machineID','errorID']\nerrorsDF = errorsDF[errorsDF.machineID != \"machineID\"]\nerrorsDF = sqlContext.createDataFrame(errorsDF)\n\n\nmaintDF = spark.read.format(\"org.apache.spark.sql.insightedge\").option(\"collection\", \"maint\").load().toPandas()\nmaintDF.columns = ['datetime','machineID','compx']\nmaintID = maintDF[maintDF.machineID != \"machineID\"]\nmaintDF = sqlContext.createDataFrame(maintDF)\n\n\ntelemetryDF = spark.read.format(\"org.apache.spark.sql.insightedge\").option(\"collection\", \"telemetry\").load().toPandas()\ntelemetryDF.columns = ['datetime','machineID','volt','rotate','pressure','vibration']\ntelemetryDF = telemetryDF[telemetryDF.machineID != \"machineID\"]\ntelemetryDF = sqlContext.createDataFrame(telemetryDF)\n\n\nfailuresDF = spark.read.format(\"org.apache.spark.sql.insightedge\").option(\"collection\", \"failures\").load().toPandas()\nfailuresDF.columns = ['datetime','machineID','failure']\nfailuresDF = failuresDF[failuresDF.machineID != \"machineID\"]\nfailuresDF = sqlContext.createDataFrame(failuresDF)\n\ntelemetry = telemetryDF\nerrors = errorsDF\nfailures = failuresDF\nmachines = machinesDF\nmaint = maintDF\n\n","user":"anonymous","dateUpdated":"2019-06-17T16:16:15+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  machineID   model age\n1         1  model2  18\n2         2  model4   7\n"}]},"apps":[],"jobName":"paragraph_1560763717120_2129481086","id":"20190320-173324_83188567","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:16:16+0300","dateFinished":"2019-06-17T16:16:19+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1885"},{"text":"%pyspark\n\n\n# rolling mean and standard deviation\n# Temporary storage for rolling means\ntel_mean = telemetryDF\n\n# Which features are we interested in telemetry data set\nrolling_features = ['volt','rotate', 'pressure', 'vibration']\n      \n# n hours = n * 3600 seconds  \ntime_val = 12 * 3600\n\n\n\n# Choose the time_val hour timestamps to align the data\n# dt_truncated looks at the column named \"datetime\" in the current data set.\ndt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val).cast(\"timestamp\"))\n\n\n\n# We choose windows for our rolling windows 12hrs, 24 hrs and 36 hrs\nlags = [12, 24, 36]\n\n# align the data\nfor lag_n in lags:\n    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n    for col_name in rolling_features:\n        tel_mean = tel_mean.withColumn(col_name+'_rollingmean_'+str(lag_n), \n                                       F.avg(col(col_name)).over(wSpec))\n        tel_mean = tel_mean.withColumn(col_name+'_rollingstd_'+str(lag_n), \n                                       F.stddev(col(col_name)).over(wSpec))\n\n# Calculate lag values...\ntelemetry_feat = (tel_mean.withColumn(\"dt_truncated\", dt_truncated)\n                  .drop('volt', 'rotate', 'pressure', 'vibration')\n                  .fillna(0)\n                  .groupBy(\"machineID\",\"dt_truncated\")\n                  .agg(F.mean('volt_rollingmean_12').alias('volt_rollingmean_12'),\n                       F.mean('rotate_rollingmean_12').alias('rotate_rollingmean_12'), \n                       F.mean('pressure_rollingmean_12').alias('pressure_rollingmean_12'), \n                       F.mean('vibration_rollingmean_12').alias('vibration_rollingmean_12'), \n                       F.mean('volt_rollingmean_24').alias('volt_rollingmean_24'),\n                       F.mean('rotate_rollingmean_24').alias('rotate_rollingmean_24'), \n                       F.mean('pressure_rollingmean_24').alias('pressure_rollingmean_24'), \n                       F.mean('vibration_rollingmean_24').alias('vibration_rollingmean_24'),\n                       F.mean('volt_rollingmean_36').alias('volt_rollingmean_36'),\n                       F.mean('vibration_rollingmean_36').alias('vibration_rollingmean_36'),\n                       F.mean('rotate_rollingmean_36').alias('rotate_rollingmean_36'), \n                       F.mean('pressure_rollingmean_36').alias('pressure_rollingmean_36'), \n                       F.stddev('volt_rollingstd_12').alias('volt_rollingstd_12'),\n                       F.stddev('rotate_rollingstd_12').alias('rotate_rollingstd_12'), \n                       F.stddev('pressure_rollingstd_12').alias('pressure_rollingstd_12'), \n                       F.stddev('vibration_rollingstd_12').alias('vibration_rollingstd_12'), \n                       F.stddev('volt_rollingstd_24').alias('volt_rollingstd_24'),\n                       F.stddev('rotate_rollingstd_24').alias('rotate_rollingstd_24'), \n                       F.stddev('pressure_rollingstd_24').alias('pressure_rollingstd_24'), \n                       F.stddev('vibration_rollingstd_24').alias('vibration_rollingstd_24'),\n                       F.stddev('volt_rollingstd_36').alias('volt_rollingstd_36'),\n                       F.stddev('rotate_rollingstd_36').alias('rotate_rollingstd_36'), \n                       F.stddev('pressure_rollingstd_36').alias('pressure_rollingstd_36'), \n                       F.stddev('vibration_rollingstd_36').alias('vibration_rollingstd_36'), ))\n\nprint(telemetry_feat.count())\ntelemetry_feat.where((col(\"machineID\") == 1)).limit(10).toPandas().head(10)\n\n\n\n","user":"anonymous","dateUpdated":"2019-06-17T16:16:19+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1464\n  machineID  ... vibration_rollingstd_36\n0         1  ...                4.044054\n1         1  ...                0.686558\n2         1  ...                0.220576\n3         1  ...                0.494304\n4         1  ...                0.094415\n5         1  ...                0.244977\n6         1  ...                0.398334\n7         1  ...                0.143925\n8         1  ...                0.262969\n9         1  ...                0.346145\n\n[10 rows x 26 columns]\n"}]},"apps":[],"jobName":"paragraph_1560763717120_-612253732","id":"20190325-175303_492660101","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:16:19+0300","dateFinished":"2019-06-17T16:16:57+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1886"},{"text":"%pyspark\n\n\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   ERRORS @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n\n# create a column for each errorID \nerror_ind = (errors.groupBy(\"machineID\",\"datetime\",\"errorID\").pivot('errorID')\n             .agg(F.count('machineID').alias('dummy')).drop('errorID').fillna(0)\n             .groupBy(\"machineID\",\"datetime\")\n             .agg(F.sum('error1').alias('error1sum'), \n                  F.sum('error2').alias('error2sum'), \n                  F.sum('error3').alias('error3sum'), \n                  F.sum('error4').alias('error4sum'), \n                  F.sum('error5').alias('error5sum')))\n                  \n                  \n\n# join the telemetry data with errors\nerror_count = (telemetry.join(error_ind, \n                              ((telemetry['machineID'] == error_ind['machineID']) \n                               & (telemetry['datetime'] == error_ind['datetime'])), \"left\")\n               .drop('volt', 'rotate', 'pressure', 'vibration')\n               .drop(error_ind.machineID).drop(error_ind.datetime)\n               .fillna(0))\n\nerror_features = ['error1sum','error2sum', 'error3sum', 'error4sum', 'error5sum']\n\nwSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-24, 0)\nfor col_name in error_features:\n    # We're only interested in the errors in the previous 24 hours.\n    error_count = error_count.withColumn(col_name+'_rollingmean_24', \n                                         F.avg(col(col_name)).over(wSpec))\n\nerror_feat = (error_count.withColumn(\"dt_truncated\", dt_truncated)\n              .drop('error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum').fillna(0)\n              .groupBy(\"machineID\",\"dt_truncated\")\n              .agg(F.mean('error1sum_rollingmean_24').alias('error1sum_rollingmean_24'), \n                   F.mean('error2sum_rollingmean_24').alias('error2sum_rollingmean_24'), \n                   F.mean('error3sum_rollingmean_24').alias('error3sum_rollingmean_24'), \n                   F.mean('error4sum_rollingmean_24').alias('error4sum_rollingmean_24'), \n                   F.mean('error5sum_rollingmean_24').alias('error5sum_rollingmean_24')))\n\nprint(error_feat.count())\nerror_feat.limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-17T16:16:57+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1464\n  machineID  ... error5sum_rollingmean_24\n0         1  ...                      0.0\n1         1  ...                      0.0\n2         1  ...                      0.0\n3         1  ...                      0.0\n4         1  ...                      0.0\n5         1  ...                      0.0\n6         1  ...                      0.0\n7         1  ...                      0.0\n8         1  ...                      0.0\n9         1  ...                      0.0\n\n[10 rows x 7 columns]\n"}]},"apps":[],"jobName":"paragraph_1560763717121_-1923369617","id":"20190325-175353_151334491","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:16:57+0300","dateFinished":"2019-06-17T16:17:09+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1887"},{"text":"%pyspark\n\n\n#using the maint dataframe now :\nmaint_replace = (maint.groupBy(\"machineID\",\"datetime\",\"compx\").pivot(\"compx\")\n                 .agg(F.count('machineID').alias('dummy')).fillna(0)\n                 .groupBy(\"machineID\",\"datetime\")\n                 .agg(F.sum('comp1').alias('comp1sum'), \n                      F.sum('comp2').alias('comp2sum'), \n                      F.sum('comp3').alias('comp3sum'),\n                      F.sum('comp4').alias('comp4sum')))\n\nmaint_replace = maint_replace.withColumnRenamed('datetime','datetime_maint')\n\nprint(maint_replace.count())\nmaint_replace.limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-17T16:17:09+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"55\n  machineID       datetime_maint  comp1sum  comp2sum  comp3sum  comp4sum\n0         1  2015-02-04 06:00:00         0         0         1         0\n1         1  2015-05-20 06:00:00         0         1         0         0\n2         2  2015-07-31 06:00:00         0         0         1         0\n3         2  2015-06-01 06:00:00         1         1         0         0\n4         1  2015-12-01 06:00:00         0         0         1         0\n5         2  2015-02-01 06:00:00         0         1         0         0\n6         1  2015-01-05 06:00:00         1         0         0         0\n7         1  2015-03-06 06:00:00         0         0         1         0\n8         2  2015-09-14 06:00:00         0         1         1         0\n9         2  2015-03-03 06:00:00         1         0         0         1\n"}]},"apps":[],"jobName":"paragraph_1560763717122_1301072282","id":"20190325-175331_1016894156","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:17:09+0300","dateFinished":"2019-06-17T16:17:13+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1888"},{"text":"%pyspark\n\n\n\n# First component number 1 (`comp1`):\n\n# We want to align the component information on telemetry features timestamps.\ntelemetry_times = (telemetry_feat.select(telemetry_feat.machineID, telemetry_feat.dt_truncated)\n                   .withColumnRenamed('dt_truncated','datetime_tel'))\n\n# Grab component 1 records\nmaint_comp1 = (maint_replace.where(col(\"comp1sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp2sum', 'comp3sum', 'comp4sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp1 = (telemetry_times.join(maint_comp1, \n                                        ((telemetry_times ['machineID']== maint_comp1['machineID']) \n                                         & (telemetry_times ['datetime_tel'] > maint_comp1['datetime_maint']) \n                                         & ( maint_comp1['comp1sum'] == '1')))\n                   .drop(maint_comp1.machineID))\n\n# Calculate the number of days between replacements\ncomp1 = (maint_tel_comp1.withColumn(\"sincelastcomp1\", \n                                    datediff(maint_tel_comp1.datetime_tel, maint_tel_comp1.datetime_maint))\n         .drop(maint_tel_comp1.datetime_maint).drop(maint_tel_comp1.comp1sum))\n\nprint(comp1.count())\ncomp1.filter(comp1.machineID == '1').orderBy(comp1.datetime_tel).limit(20).toPandas().head(20)\n\n\n","user":"anonymous","dateUpdated":"2019-06-17T16:17:13+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"7628\n   machineID        datetime_tel  sincelastcomp1\n0          1 2015-01-01 02:00:00             109\n1          1 2015-01-01 14:00:00             109\n2          1 2015-01-02 02:00:00             110\n3          1 2015-01-02 14:00:00             110\n4          1 2015-01-03 02:00:00             111\n5          1 2015-01-03 14:00:00             111\n6          1 2015-01-04 02:00:00             112\n7          1 2015-01-04 14:00:00             112\n8          1 2015-01-05 02:00:00             113\n9          1 2015-01-05 14:00:00               0\n10         1 2015-01-05 14:00:00             113\n11         1 2015-01-06 02:00:00               1\n12         1 2015-01-06 02:00:00             114\n13         1 2015-01-06 14:00:00             114\n14         1 2015-01-06 14:00:00               1\n15         1 2015-01-07 02:00:00               2\n16         1 2015-01-07 02:00:00             115\n17         1 2015-01-07 14:00:00             115\n18         1 2015-01-07 14:00:00               2\n19         1 2015-01-08 02:00:00               3\n"}]},"apps":[],"jobName":"paragraph_1560763717123_1065809464","id":"20190325-143516_603335218","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:17:13+0300","dateFinished":"2019-06-17T16:17:27+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1889"},{"text":"%pyspark\n\n# Then component 2 (`comp2`):\n\n# Grab component 2 records\nmaint_comp2 = (maint_replace.where(col(\"comp2sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp1sum', 'comp3sum', 'comp4sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp2 = (telemetry_times.join(maint_comp2, \n                                        ((telemetry_times ['machineID']== maint_comp2['machineID']) \n                                         & (telemetry_times ['datetime_tel'] > maint_comp2['datetime_maint']) \n                                         & ( maint_comp2['comp2sum'] == '1')))\n                   .drop(maint_comp2.machineID))\n\n# Calculate the number of days between replacements\ncomp2 = (maint_tel_comp2.withColumn(\"sincelastcomp2\", \n                                    datediff(maint_tel_comp2.datetime_tel, maint_tel_comp2.datetime_maint))\n         .drop(maint_tel_comp2.datetime_maint).drop(maint_tel_comp2.comp2sum))\n\nprint(comp2.count())\ncomp2.filter(comp2.machineID == '1').orderBy(comp2.datetime_tel).limit(5).toPandas().head(5)\n\n\n","user":"anonymous","dateUpdated":"2019-06-17T16:17:27+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"5600\n  machineID        datetime_tel  sincelastcomp2\n0         1 2015-01-01 02:00:00             109\n1         1 2015-01-01 14:00:00             109\n2         1 2015-01-02 02:00:00             110\n3         1 2015-01-02 14:00:00             110\n4         1 2015-01-03 02:00:00             111\n"}]},"apps":[],"jobName":"paragraph_1560763717123_-1192825608","id":"20190325-164418_253310820","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:17:27+0300","dateFinished":"2019-06-17T16:17:38+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1890"},{"text":"%pyspark\n\n# Then component 3 (`comp3`):\n\n# Grab component 3 records\nmaint_comp3 = (maint_replace.where(col(\"comp3sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp1sum', 'comp2sum', 'comp4sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp3 = (telemetry_times.join(maint_comp3, ((telemetry_times ['machineID']==maint_comp3['machineID']) \n                                                      & (telemetry_times ['datetime_tel'] > maint_comp3['datetime_maint']) \n                                                      & ( maint_comp3['comp3sum'] == '1')))\n                   .drop(maint_comp3.machineID))\n\n# Calculate the number of days between replacements\ncomp3 = (maint_tel_comp3.withColumn(\"sincelastcomp3\", \n                                    datediff(maint_tel_comp3.datetime_tel, maint_tel_comp3.datetime_maint))\n         .drop(maint_tel_comp3.datetime_maint).drop(maint_tel_comp3.comp3sum))\n\n\nprint(comp3.count())\ncomp3.filter(comp3.machineID == '1').orderBy(comp3.datetime_tel).limit(5).toPandas().head(5)","user":"anonymous","dateUpdated":"2019-06-17T16:17:38+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"8324\n  machineID        datetime_tel  sincelastcomp3\n0         1 2015-01-01 02:00:00              49\n1         1 2015-01-01 14:00:00              49\n2         1 2015-01-02 02:00:00              50\n3         1 2015-01-02 14:00:00              50\n4         1 2015-01-03 02:00:00              51\n"}]},"apps":[],"jobName":"paragraph_1560763717124_836660462","id":"20190325-164930_1877130615","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:17:38+0300","dateFinished":"2019-06-17T16:17:48+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1891"},{"text":"%pyspark\n\n# and component 4 (`comp4`):\n\n# Grab component 4 records\nmaint_comp4 = (maint_replace.where(col(\"comp4sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp1sum', 'comp2sum', 'comp3sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp4 = telemetry_times.join(maint_comp4, ((telemetry_times['machineID']==maint_comp4['machineID']) \n                                                     & (telemetry_times['datetime_tel'] > maint_comp4['datetime_maint']) \n                                                     & (maint_comp4['comp4sum'] == '1'))).drop(maint_comp4.machineID)\n\n# Calculate the number of days between replacements\ncomp4 = (maint_tel_comp4.withColumn(\"sincelastcomp4\", \n                                    datediff(maint_tel_comp4.datetime_tel, maint_tel_comp4.datetime_maint))\n         .drop(maint_tel_comp4.datetime_maint).drop(maint_tel_comp4.comp4sum))\n\nprint(comp4.count())\ncomp4.filter(comp4.machineID == '1').orderBy(comp4.datetime_tel).limit(5).toPandas().head(5)\n","user":"anonymous","dateUpdated":"2019-06-17T16:17:48+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"6054\n  machineID        datetime_tel  sincelastcomp4\n0         1 2015-01-01 02:00:00             184\n1         1 2015-01-01 14:00:00             184\n2         1 2015-01-02 02:00:00             185\n3         1 2015-01-02 14:00:00             185\n4         1 2015-01-03 02:00:00             186\n"}]},"apps":[],"jobName":"paragraph_1560763717125_96531502","id":"20190325-165022_36704918","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:17:48+0300","dateFinished":"2019-06-17T16:17:56+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1892"},{"text":"%pyspark\n# Now we join the four component replacement tables together.\n\n\n# Join component 3 and 4\ncomp3_4 = (comp3.join(comp4, ((comp3['machineID'] == comp4['machineID']) \n                              & (comp3['datetime_tel'] == comp4['datetime_tel'])), \"left\")\n           .drop(comp4.machineID).drop(comp4.datetime_tel))\n\n# Join component 2 to 3 and 4\ncomp2_3_4 = (comp2.join(comp3_4, ((comp2['machineID'] == comp3_4['machineID']) \n                                  & (comp2['datetime_tel'] == comp3_4['datetime_tel'])), \"left\")\n             .drop(comp3_4.machineID).drop(comp3_4.datetime_tel))\n\n# Join component 1 to 2, 3 and 4\ncomps_feat = (comp1.join(comp2_3_4, ((comp1['machineID'] == comp2_3_4['machineID']) \n                                      & (comp1['datetime_tel'] == comp2_3_4['datetime_tel'])), \"left\")\n               .drop(comp2_3_4.machineID).drop(comp2_3_4.datetime_tel)\n               .groupBy(\"machineID\", \"datetime_tel\")\n               .agg(F.max('sincelastcomp1').alias('sincelastcomp1'), \n                    F.max('sincelastcomp2').alias('sincelastcomp2'), \n                    F.max('sincelastcomp3').alias('sincelastcomp3'), \n                    F.max('sincelastcomp4').alias('sincelastcomp4'))\n               .fillna(0))\n\n# Choose the time_val hour timestamps to align the data\ndt_truncated = ((round(unix_timestamp(col(\"datetime_tel\")) / time_val) * time_val).cast(\"timestamp\"))\n\n# Collect data\nmaint_feat = (comps_feat.withColumn(\"dt_truncated\", dt_truncated)\n              .groupBy(\"machineID\",\"dt_truncated\")\n              .agg(F.mean('sincelastcomp1').alias('comp1sum'), \n                   F.mean('sincelastcomp2').alias('comp2sum'), \n                   F.mean('sincelastcomp3').alias('comp3sum'), \n                   F.mean('sincelastcomp4').alias('comp4sum')))\n\nprint(maint_feat.count())\nmaint_feat.limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-18T11:29:39+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1462\n  machineID        dt_truncated  comp1sum  comp2sum  comp3sum  comp4sum\n0         1 2015-01-12 14:00:00     120.0     120.0      60.0     195.0\n1         1 2015-06-03 15:00:00     262.0     262.0     202.0     337.0\n2         1 2015-09-19 15:00:00     370.0     370.0     310.0     445.0\n3         2 2015-02-06 14:00:00     220.0      85.0     115.0     190.0\n4         2 2015-03-15 14:00:00     257.0     122.0     152.0     227.0\n5         2 2015-12-07 02:00:00     524.0     389.0     419.0     494.0\n6         1 2015-02-18 14:00:00     157.0     157.0      97.0     232.0\n7         1 2015-10-01 15:00:00     382.0     382.0     322.0     457.0\n8         1 2015-12-12 02:00:00     454.0     454.0     394.0     529.0\n9         2 2015-02-07 02:00:00     221.0      86.0     116.0     191.0\n"}]},"apps":[],"jobName":"paragraph_1560763717125_1585649542","id":"20190325-165139_680665999","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:17:56+0300","dateFinished":"2019-06-17T16:18:36+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1893"},{"text":"%pyspark\n\n\n# ## Machine features\n\n# one hot encoding of the variable model, basically creates a set of dummy boolean variables\ncatVarNames = ['model']  \nsIndexers = [StringIndexer(inputCol=x, outputCol=x + '_indexed') for x in catVarNames]\nmachines_cat = Pipeline(stages=sIndexers).fit(machines).transform(machines)\nprint(machines_cat)\n\n# one-hot encoder\nohEncoders = [OneHotEncoder(inputCol=x + '_indexed', outputCol=x + '_encoded')\n              for x in catVarNames]\n              \n\ntmp1 = Pipeline(stages=ohEncoders)\ntmp2 = tmp1.fit(machines_cat)\nprint(tmp2)\nmachines_cat = tmp2.transform(machines_cat)\n\n\ndrop_list = [col_n for col_n in machines_cat.columns if 'indexed' in col_n]\n\nmachines_feat = machines_cat.select([column for column in machines_cat.columns if column not in drop_list])\n\nprint(machines_feat.count())\nmachines_feat.limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-17T16:18:36+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DataFrame[machineID: string, model: string, age: string, model_indexed: double]\nPipelineModel_7aa8a26491a6\n2\n  machineID   model age model_encoded\n0         1  model2  18         (1.0)\n1         2  model4   7         (0.0)\n"}]},"apps":[],"jobName":"paragraph_1560763717126_592561302","id":"20190325-165255_426981307","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:18:36+0300","dateFinished":"2019-06-17T16:18:38+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1894"},{"text":"%pyspark\n\n\n# ## Merging feature data\n# Next, we merge the telemetry, maintenance, machine and error feature data sets into a large feature data set.\n# Since most of the data has already been aligned on the 12 hour observation period, we can merge with a join.\n\n# join error features with component maintenance features\nerror_maint = (error_feat.join(maint_feat, \n                               ((error_feat['machineID'] == maint_feat['machineID']) \n                                & (error_feat['dt_truncated'] == maint_feat['dt_truncated'])), \"left\")\n               .drop(maint_feat.machineID).drop(maint_feat.dt_truncated))\n\n# now join that with machines features\nerror_maint_feat = (error_maint.join(machines_feat, \n                                     ((error_maint['machineID'] == machines_feat['machineID'])), \"left\")\n                    .drop(machines_feat.machineID))\n\n# Clean up some unecessary columns\nerror_maint_feat = error_maint_feat.select([c for c in error_maint_feat.columns if c not in \n                                            {'error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum'}])\n\n# join telemetry with error/maint/machine features to create final feature matrix\nfinal_feat = (telemetry_feat.join(error_maint_feat, \n                                  ((telemetry_feat['machineID'] == error_maint_feat['machineID']) \n                                   & (telemetry_feat['dt_truncated'] == error_maint_feat['dt_truncated'])), \"left\")\n              .drop(error_maint_feat.machineID).drop(error_maint_feat.dt_truncated))\n\nprint(final_feat.count())\nfinal_feat.filter(final_feat.machineID == '1').orderBy(final_feat.dt_truncated).limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-17T16:18:38+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"1464\n"},{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o1031.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 144 in stage 224.0 failed 1 times, most recent failure: Lost task 144.0 in stage 224.0 (TID 19837, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: null\n\nPrevious exception in task: Unable to acquire 16384 bytes of memory, got 0\n\torg.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\torg.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:98)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.reset(UnsafeInMemorySorter.java:186)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:229)\n\torg.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:206)\n\torg.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:285)\n\torg.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:383)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:407)\n\torg.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:142)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.fetchNextPartition(WindowExec.scala:343)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.next(WindowExec.scala:369)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.next(WindowExec.scala:303)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)\n\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.fetchNextRow(WindowExec.scala:314)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.<init>(WindowExec.scala:323)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11.apply(WindowExec.scala:303)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11.apply(WindowExec.scala:302)\n\torg.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\torg.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\torg.apache.spark.scheduler.Task.run(Task.scala:121)\n\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tjava.lang.Thread.run(Thread.java:748)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: null\n\nPrevious exception in task: Unable to acquire 16384 bytes of memory, got 0\n\torg.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\torg.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:98)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.reset(UnsafeInMemorySorter.java:186)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:229)\n\torg.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:206)\n\torg.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:285)\n\torg.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:383)\n\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:407)\n\torg.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:142)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.fetchNextPartition(WindowExec.scala:343)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.next(WindowExec.scala:369)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.next(WindowExec.scala:303)\n\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)\n\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.fetchNextRow(WindowExec.scala:314)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.<init>(WindowExec.scala:323)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11.apply(WindowExec.scala:303)\n\torg.apache.spark.sql.execution.window.WindowExec$$anonfun$11.apply(WindowExec.scala:302)\n\torg.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\torg.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\torg.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\torg.apache.spark.scheduler.Task.run(Task.scala:121)\n\torg.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tjava.lang.Thread.run(Thread.java:748)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o1031.collectToPython.\\n', JavaObject id=o1038), <traceback object at 0x7fe9d8b00200>)"}]},"apps":[],"jobName":"paragraph_1560763717127_-1506409619","id":"20190325-165358_406247828","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T16:18:38+0300","dateFinished":"2019-06-17T16:19:51+0300","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1895"},{"text":"%pyspark\n\n\n\n\n# # Label construction\n\n# For this example scenerio, we estimate the probability that a machine will fail in the near future due to a failure of a certain component. More specifically, the goal is to compute the probability that a machine will fail in the next 7 days due to a component failure (component 1, 2, 3, or 4). \n# \n# Below, a categorical failure feature is created to serve as the label. All records within a 24 hour window before a failure of component 1 have failure=\"comp1\", and so on for components 2, 3, and 4; all records not within 7 days of a component failure have failure=\"none\".\n# \n# The first step is to align the failure data to the feature observation time points (every 12 hours).\n\n\ndt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val).cast(\"timestamp\"))\n\nfail_diff = (failures.withColumn(\"dt_truncated\", dt_truncated)\n             .drop(failures.datetime))\n\nprint(fail_diff.count())\nfail_diff.limit(10).toPandas().head(10)\n\n\n","user":"anonymous","dateUpdated":"2019-06-17T15:25:41+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"19\n  machineID failure        dt_truncated\n0         2   comp1 2015-09-29 03:00:00\n1         2   comp2 2015-04-02 03:00:00\n2         1   comp1 2015-11-16 02:00:00\n3         1   comp2 2015-06-19 03:00:00\n4         1   comp4 2015-04-05 03:00:00\n5         1   comp3 2015-08-03 03:00:00\n6         1   comp3 2015-05-05 03:00:00\n7         2   comp1 2015-04-17 03:00:00\n8         1   comp4 2015-12-16 02:00:00\n9         2   comp2 2015-06-01 03:00:00\n"}]},"apps":[],"jobName":"paragraph_1560763717127_1592713997","id":"20190325-175854_1414495642","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T15:25:41+0300","dateFinished":"2019-06-17T15:25:41+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1896"},{"text":"%pyspark\n\n\n# Next, we convert the labels from text to numeric values. In the end, this will transform the problem from boolean of 'healthy'/'impending failure' to a multiclass 'healthy'/'component `n` impending failure'.\n\n# map the failure data to final feature matrix\nlabeled_features = (final_feat.join(fail_diff, \n                                    ((final_feat['machineID'] == fail_diff['machineID']) \n                                     & (final_feat['dt_truncated'] == fail_diff['dt_truncated'])), \"left\")\n                    .drop(fail_diff.machineID).drop(fail_diff.dt_truncated)\n                    .withColumn('failure', F.when(col('failure') == \"comp1\", 1.0).otherwise(col('failure')))\n                    .withColumn('failure', F.when(col('failure') == \"comp2\", 2.0).otherwise(col('failure')))\n                    .withColumn('failure', F.when(col('failure') == \"comp3\", 3.0).otherwise(col('failure')))\n                    .withColumn('failure', F.when(col('failure') == \"comp4\", 4.0).otherwise(col('failure'))))\n\nlabeled_features = (labeled_features.withColumn(\"failure\", \n                                                labeled_features.failure.cast(DoubleType()))\n                    .fillna(0))\n\nprint(labeled_features.count())\nlabeled_features.limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-17T15:25:41+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1465\n         dt_truncated  volt_rollingmean_12  ...  model_encoded  failure\n0 2015-01-12 14:00:00           168.142435  ...          (1.0)      0.0\n1 2015-06-03 15:00:00           173.395542  ...          (1.0)      0.0\n2 2015-09-19 15:00:00           172.866894  ...          (1.0)      0.0\n3 2015-02-06 14:00:00           166.094641  ...          (0.0)      0.0\n4 2015-03-15 14:00:00           177.100000  ...          (0.0)      0.0\n5 2015-12-07 02:00:00           171.405667  ...          (0.0)      0.0\n6 2015-02-18 14:00:00           172.483220  ...          (1.0)      0.0\n7 2015-10-01 15:00:00           170.632940  ...          (1.0)      0.0\n8 2015-12-12 02:00:00           170.426215  ...          (1.0)      0.0\n9 2015-02-07 02:00:00           169.488959  ...          (0.0)      0.0\n\n[10 rows x 39 columns]\n"}]},"apps":[],"jobName":"paragraph_1560763717128_324787731","id":"20190325-180022_441485164","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T15:25:41+0300","dateFinished":"2019-06-17T15:27:43+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1897"},{"text":"%pyspark\n\n# To verify we have assigned the component failure records correctly, we count the failure classes within the feature data.\n# To get the frequency of each component failure \ndf = labeled_features.select(labeled_features.failure).toPandas()\ndf['failure'].value_counts()\n\n\n","user":"anonymous","dateUpdated":"2019-06-17T15:27:43+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"0.0    1446\n2.0       6\n1.0       5\n4.0       5\n3.0       3\nName: failure, dtype: int64\n"}]},"apps":[],"jobName":"paragraph_1560763717128_920176312","id":"20190325-180048_203897485","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T15:27:43+0300","dateFinished":"2019-06-17T15:28:28+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1898"},{"text":"%pyspark\n\n\n# To now, we have labels as _failure events_. To convert to _impending failure_, we over label over the previous 7 days as _failed_.\n# lag values to manually backfill label (bfill =7)\nmy_window = Window.partitionBy('machineID').orderBy(labeled_features.dt_truncated.desc())\n\n# Create the previous 7 days \nlabeled_features = (labeled_features.withColumn(\"prev_value1\", \n                                                F.lag(labeled_features.failure).\n                                                over(my_window)).fillna(0))\nlabeled_features = (labeled_features.withColumn(\"prev_value2\", \n                                                F.lag(labeled_features.prev_value1).\n                                                over(my_window)).fillna(0))\nlabeled_features = (labeled_features.withColumn(\"prev_value3\", \n                                                F.lag(labeled_features.prev_value2).\n                                                over(my_window)).fillna(0))\nlabeled_features = (labeled_features.withColumn(\"prev_value4\", \n                                                F.lag(labeled_features.prev_value3).\n                                                over(my_window)).fillna(0)) \nlabeled_features = (labeled_features.withColumn(\"prev_value5\", \n                                                F.lag(labeled_features.prev_value4).\n                                                over(my_window)).fillna(0)) \nlabeled_features = (labeled_features.withColumn(\"prev_value6\", \n                                                F.lag(labeled_features.prev_value5).\n                                                over(my_window)).fillna(0))\nlabeled_features = (labeled_features.withColumn(\"prev_value7\", \n                                                F.lag(labeled_features.prev_value6).\n                                                over(my_window)).fillna(0))\n\n# Create a label features\nlabeled_features = (labeled_features.withColumn('label', labeled_features.failure + \n                                                labeled_features.prev_value1 +\n                                                labeled_features.prev_value2 +\n                                                labeled_features.prev_value3 +\n                                                labeled_features.prev_value4 +\n                                                labeled_features.prev_value5 + \n                                                labeled_features.prev_value6 + \n                                                labeled_features.prev_value7))\n\n# Restrict the label to be on the range of 0:4, and remove extra columns\nlabeled_features = (labeled_features.withColumn('label_e', F.when(col('label') > 4, 4.0)\n                                                .otherwise(col('label')))\n                    .drop(labeled_features.prev_value1).drop(labeled_features.prev_value2)\n                    .drop(labeled_features.prev_value3).drop(labeled_features.prev_value4)\n                    .drop(labeled_features.prev_value5).drop(labeled_features.prev_value6)\n                    .drop(labeled_features.prev_value7).drop(labeled_features.label))\n\nprint(labeled_features.count())\nlabeled_features.limit(10).toPandas().head(10)\n","user":"anonymous","dateUpdated":"2019-06-17T15:28:28+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1465\n         dt_truncated  volt_rollingmean_12  ...  failure  label_e\n0                 NaT           171.185990  ...      0.0      0.0\n1                 NaT           173.639982  ...      0.0      0.0\n2 2016-01-01 02:00:00           171.770114  ...      0.0      0.0\n3 2015-12-31 14:00:00           166.886041  ...      0.0      0.0\n4 2015-12-31 02:00:00           163.640499  ...      0.0      0.0\n5 2015-12-30 14:00:00           164.603472  ...      0.0      0.0\n6 2015-12-30 02:00:00           167.086896  ...      0.0      0.0\n7 2015-12-29 14:00:00           167.114825  ...      0.0      0.0\n8 2015-12-29 02:00:00           168.882617  ...      0.0      0.0\n9 2015-12-28 14:00:00           168.629767  ...      0.0      0.0\n\n[10 rows x 40 columns]\n"}]},"apps":[],"jobName":"paragraph_1560763717129_-1653613127","id":"20190325-180324_1920739180","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T15:28:28+0300","dateFinished":"2019-06-17T15:30:40+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1899"},{"text":"%pyspark\n\n\n\n# To verify the label construction, we plot a sample of four machines over the data set life time. We expect the labels to cluster for each component, since there are 7 day windows of \"fail\". We have omitted the healthy labels, as they are uninformative. Since the labels are actually classes, the plot as four distinct values on the y-axis.\n\n\nplt_dta = (labeled_features.filter(labeled_features.label_e > 0)\n           .where(col(\"machineID\").isin({\"1\", \"2\", \"222\", \"965\"}))\n           .select(labeled_features.machineID, labeled_features.dt_truncated, labeled_features.label_e)\n           .toPandas())\n           \nprint(plt_dta)\n\n# format datetime field which comes in as string\nplt_dta['dt_truncated'] = pd.to_datetime(plt_dta['dt_truncated'], format=\"%Y-%m-%d %H:%M:%S\")\nplt_dta.label_e = plt_dta.label_e.astype(int)\n\n#ggplot(aes(x=\"dt_truncated\", y=\"label_e\", color=\"label_e\"), plt_dta) +    geom_point()+    xlab(\"Date\") + ylab(\"Component Number\") +    scale_x_date(labels=date_format('%m-%d')) +    scale_color_brewer(type = 'seq', palette = 'BuGn') +    facet_grid('machineID')\n\n\n","user":"anonymous","dateUpdated":"2019-06-17T15:30:40+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"    machineID        dt_truncated  label_e\n0           1 2015-12-16 02:00:00      4.0\n1           1 2015-12-15 14:00:00      4.0\n2           1 2015-12-15 02:00:00      4.0\n3           1 2015-12-14 14:00:00      4.0\n4           1 2015-12-14 02:00:00      4.0\n5           1 2015-12-13 14:00:00      4.0\n6           1 2015-12-13 02:00:00      4.0\n7           1 2015-12-12 14:00:00      4.0\n8           1 2015-11-16 02:00:00      1.0\n9           1 2015-11-15 14:00:00      1.0\n10          1 2015-11-15 02:00:00      1.0\n11          1 2015-11-14 14:00:00      1.0\n12          1 2015-11-14 02:00:00      1.0\n13          1 2015-11-13 14:00:00      1.0\n14          1 2015-11-13 02:00:00      1.0\n15          1 2015-11-12 14:00:00      1.0\n16          1 2015-11-01 02:00:00      4.0\n17          1 2015-10-31 14:00:00      4.0\n18          1 2015-10-31 02:00:00      4.0\n19          1 2015-10-30 14:00:00      4.0\n20          1 2015-10-30 02:00:00      4.0\n21          1 2015-10-29 14:00:00      4.0\n22          1 2015-10-29 02:00:00      4.0\n23          1 2015-10-28 14:00:00      4.0\n24          1 2015-08-03 03:00:00      3.0\n25          1 2015-08-03 03:00:00      4.0\n26          1 2015-08-02 15:00:00      4.0\n27          1 2015-08-02 03:00:00      4.0\n28          1 2015-08-01 15:00:00      4.0\n29          1 2015-08-01 03:00:00      4.0\n..        ...                 ...      ...\n110         2 2015-05-29 15:00:00      2.0\n111         2 2015-05-29 03:00:00      2.0\n112         2 2015-05-28 15:00:00      2.0\n113         2 2015-04-17 03:00:00      1.0\n114         2 2015-04-16 15:00:00      1.0\n115         2 2015-04-16 03:00:00      1.0\n116         2 2015-04-15 15:00:00      1.0\n117         2 2015-04-15 03:00:00      1.0\n118         2 2015-04-14 15:00:00      1.0\n119         2 2015-04-14 03:00:00      1.0\n120         2 2015-04-13 15:00:00      1.0\n121         2 2015-04-02 03:00:00      2.0\n122         2 2015-04-01 15:00:00      2.0\n123         2 2015-04-01 03:00:00      2.0\n124         2 2015-03-31 15:00:00      2.0\n125         2 2015-03-31 03:00:00      2.0\n126         2 2015-03-30 15:00:00      2.0\n127         2 2015-03-30 03:00:00      2.0\n128         2 2015-03-29 15:00:00      2.0\n129         2 2015-02-01 02:00:00      2.0\n130         2 2015-01-31 14:00:00      2.0\n131         2 2015-01-31 02:00:00      2.0\n132         2 2015-01-30 14:00:00      2.0\n133         2 2015-01-30 02:00:00      2.0\n134         2 2015-01-29 14:00:00      2.0\n135         2 2015-01-29 02:00:00      2.0\n136         2 2015-01-28 14:00:00      2.0\n137         2 2015-01-02 02:00:00      2.0\n138         2 2015-01-01 14:00:00      2.0\n139         2 2015-01-01 02:00:00      2.0\n\n[140 rows x 3 columns]\n"}]},"apps":[],"jobName":"paragraph_1560763717130_2136459311","id":"20190325-180429_1010375911","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T15:30:40+0300","dateFinished":"2019-06-17T15:31:21+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1900"},{"text":"%pyspark\n\n\nlabeled_features.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(\"features\")","user":"anonymous","dateUpdated":"2019-06-17T15:31:21+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1560763717134_464555144","id":"20190325-180551_225857500","dateCreated":"2019-06-17T12:28:37+0300","dateStarted":"2019-06-17T15:31:21+0300","dateFinished":"2019-06-17T15:32:51+0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1901"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2019-06-17T15:32:51+0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560763717136_219063506","id":"20190325-181433_1332378471","dateCreated":"2019-06-17T12:28:37+0300","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1902"}],"name":"PM","id":"2ECMUY42Z","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}