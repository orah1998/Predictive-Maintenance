{"paragraphs":[{"text":"%pyspark\n\n\n'''\nThis section is an extra, is this section, we added code that gets 4 csv files - maint, machines, errors and telemetry.\nthen, it uses some of the code (but not all of it) to create feature vectors, to be fed to the machine.\n\n** make sure that the files consists of more than a few machine description.\n\n\n'''\n\n\n\n## Setup our environment by importing required libraries\nimport time\nimport os\nimport glob\nimport urllib\n\n# Read csv file from URL directly\nimport pandas as pd\n\n\n# For creating some preliminary EDA plots.\n#get_ipython().magic(u'matplotlib inline')\nimport matplotlib.pyplot as plt\n#from ggplot import *\n\nfrom datetime import datetime\n\n# Setup the pyspark environment\nfrom pyspark.sql import SparkSession\n\n# For logging model evaluation parameters back into the\n# AML Workbench run history plots.\nimport logging\n\namllog = logging.getLogger(\"azureml\")\namllog.level = logging.INFO\n\n\n# Time the notebook execution. \n# This will only make sense if you \"Run All\" cells\ntic = time.time()\n\nspark = SparkSession.builder.getOrCreate()\nCONTAINER_NAME = \"dataingestion\"\n\n# The raw data is stored on GitHub here:\nbasedataurl = \"http://media.githubusercontent.com/media/Microsoft/SQL-Server-R-Services-Samples/master/PredictiveMaintanenceModelingGuide/Data/\"\nMACH_DATA = 'machines_files.parquet'\nMAINT_DATA = 'maint_files.parquet'\nERROR_DATA = 'errors_files.parquet'\nTELEMETRY_DATA = 'telemetry_files.parquet'\nFAILURE_DATA = 'failure_files.parquet'\n\n\n# ### Machines data set\n\n# load raw data from the GitHub URL\ndatafile = \"machines.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nmachines = pd.read_csv(datafile, encoding='utf-8')\n\nprint(machines.count())\nmachines.head(10)\nmach_spark = spark.createDataFrame(machines, verifySchema=False)\ndel machines\n\n# Check data type conversions.\nmach_spark.printSchema()\n\n\n# Now we write the spark dataframe to an Azure blob storage container for use in the remaining notebooks of this scenario.\n\n# Write the Machine data set to intermediate storage\nmach_spark.write.mode('overwrite').parquet(MACH_DATA)\n#mach_spark.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(MACH_DATA)\n\nprint(\"Machines files saved!\\n\\n\\n\\n\\n\\n\\n\")\n\n\n# ### Errors  data set\n# \n# The error log contains non-breaking errors recorded while the machine is still operational. These errors are not considered failures, though they may be predictive of a future failure event. The error datetime field is rounded to the closest hour since the telemetry data (loaded later) is collected on an hourly rate.\n\n\n\n# load raw data from the GitHub URL\ndatafile = \"errors.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nerrors = pd.read_csv(datafile, encoding='utf-8')\n\nprint(errors.count())\nerrors.head(10)\n\n\n# The error data consists of a time series (datetime stamped) of error codes thrown by each machine (machineID). The figure shows how many errors occured in each of the five error classes over the entire year. We could split this figure over each individual machine, but with 1000 individuals, the figure would not be very informative.\n# \n# Next, we convert the errors data to a Spark dataframe, and verify the data types have converted correctly. \n\n# The data was read in using a Pandas data frame. We'll convert it to pyspark to ensure it is in a Spark usable form for later manipulations.\n\nerror_spark = spark.createDataFrame(errors, verifySchema=False)\n\n# We no longer need the pandas dataframe, so we can release that memory.\ndel errors\n\n# Check data type conversions.\nerror_spark.printSchema()\n\n\n# Now we write the spark dataframe to an Azure blob storage container for use in the remaining notebooks of this scenario.\n\n# Write the Errors data set to intermediate storage\nerror_spark.write.mode('overwrite').parquet(ERROR_DATA)\n#error_spark.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(ERROR_DATA)\n\nprint(\"Errors files saved!\")\n\n\n# ### Maintenance data set\n# \n# The maintenance log contains both scheduled and unscheduled maintenance records. Scheduled maintenance corresponds with  regular inspection of components, unscheduled maintenance may arise from mechanical failure or other performance degradations. A failure record is generated for component replacement in the case  of either maintenance events. Because maintenance events can also be used to infer component life, the maintenance data has been collected over two years (2014, 2015) instead of only over the year of interest (2015).\n\n# load raw data from the GitHub URL\ndatafile = \"maint.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nmaint = pd.read_csv(datafile, encoding='utf-8')\n\nprint(maint.count())\nmaint.head(20)\n\n# There are many ways we might want to look at this data including calculating how long each component type lasts, or the time history of component replacements within each machine. This will take some preprocess of the data, which we are delaying until we do the feature engineering steps in the next example notebook.\n# \n# Next, we convert the errors data to a Spark dataframe, and verify the data types have converted correctly. \n\n\n\n\n# The data was read in using a Pandas data frame. We'll convert it to pyspark to ensure it is in a Spark usable form for later manipulations.\nmaint_spark = spark.createDataFrame(maint, verifySchema=False)\n\n# We no longer need the pandas dataframe, so we can release that memory.\ndel maint\n\n# Check data type conversions.\nmaint_spark.printSchema()\n\n\n# Now we write the spark dataframe to an Azure blob storage container for use in the remaining notebooks of this scenario.\n\n\n# Write the Maintenance data set to intermediate storage\nmaint_spark.write.mode('overwrite').parquet(MAINT_DATA)\n#maint_spark.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(MAINT_DATA)\nprint(\"Maintenance files saved!\")\n\n\n# ### Telemetry data set\n# \n# The telemetry time-series data consists of voltage, rotation, pressure, and vibration sensor measurements collected from each  machines in real time. The data is averaged over an hour and stored in the telemetry logs.\n\n\n\n\nimport socket\nsocket.setdefaulttimeout(30)\n\n# load raw data from the GitHub URL\ndatafile = \"telemetry.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\ntelemetry = pd.read_csv(datafile, encoding='utf-8')\n\n# handle missing values\n# define groups of features \nfeatures_datetime = ['datetime']\nfeatures_categorical = ['machineID']\nfeatures_numeric = list(set(telemetry.columns) - set(features_datetime) - set(features_categorical))\n\n# Replace numeric NA with 0\ntelemetry[features_numeric] = telemetry[features_numeric].fillna(0)\n\n# Replace categorical NA with 'Unknown'\ntelemetry[features_categorical]  = telemetry[features_categorical].fillna(\"Unknown\")\n\n# Counts...\nprint(telemetry.count())\n\n# Examine 10 rowzs of data.\ntelemetry.head(10)\n\n\n# Check the incoming schema, we want to convert datetime to the correct type.\n# format datetime field which comes in as string\ntelemetry.dtypes\n\n\n\n\n\n# completely optional :\n'''\nplt_data = telemetry.loc[telemetry['machineID'] == 1]\n\n# format datetime field which comes in as string\nplt_data['datetime'] = pd.to_datetime(plt_data['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n\n\n# Quick plot to show structure\nplot_df = plt_data.loc[(plt_data['datetime'] >= pd.to_datetime('2015-02-01')) &\n                       (plt_data['datetime'] <= pd.to_datetime('2015-03-01'))]\n\nplt_data = pd.melt(plot_df, id_vars=['datetime', 'machineID'])\n\n#ggplot(aes(x=\"datetime\", y=\"value\", color = \"variable\", group=\"variable\"), plt_data) +    geom_line() +    scale_x_date(labels=date_format('%m-%d')) +    facet_grid('variable', scales='free_y')\n'''\n\n\n\n# The figure shows one month worth of telemetry sensor data for one machine. Each sensor is shown in it's own panel.\n# \n# Next, we convert the errors data to a Spark dataframe, and verify the data types have converted correctly. \n\n# The data was read in using a Pandas data frame. We'll convert it to pyspark to ensure it is in a Spark usable form for later manipulations.\n# This line takes about 9.5 minutes to run.\ntelemetry_spark = spark.createDataFrame(telemetry, verifySchema=False)\n\n# We no longer need the pandas dataframes, so we can release that memory.\ndel telemetry\ndel plt_data\ndel plot_df\n\n# Check data type conversions.\ntelemetry_spark.printSchema()\n\n\n# Now we write the spark dataframe to an Azure blob storage container for use in the remaining notebooks of this scenario.\n\n# Write the telemetry data set to intermediate storage\ntelemetry_spark.write.mode('overwrite').parquet(TELEMETRY_DATA)\n#telemetry_spark.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(TELEMETRY_DATA)\nprint(\"Telemetry files saved!\")\n\n\n\n\n\n\n\n\n\n#                                       now for engineering:\n\n\n\n\n\nimport glob\n\n# Read csv file from URL directly\nimport pandas as pd\n\n# For creating some preliminary EDA plots.\nimport matplotlib.pyplot as plt\n#from ggplot import *\n\nimport datetime\nfrom pyspark.sql.functions import to_date\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import col, unix_timestamp, round\nfrom pyspark.sql.functions import datediff\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import DoubleType\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import StringIndexer\n\nfrom pyspark.sql import SparkSession\n\n# For Azure blob storage access\nfrom azure.storage.blob import BlockBlobService\nfrom azure.storage.blob import PublicAccess\n\n# For logging model evaluation parameters back into the\n# AML Workbench run history plots.\nimport logging\n\n# Time the notebook execution. \n# This will only make sense if you \"Run all cells\"\ntic = time.time()\n\nspark = SparkSession.builder.getOrCreate()\n\n\n# These file names detail which blob each file is stored under. \nMACH_DATA = 'machines_files.parquet'\nMAINT_DATA = 'maint_files.parquet'\nERROR_DATA = 'errors_files.parquet'\nTELEMETRY_DATA = 'telemetry_files.parquet'\nFAILURE_DATA = 'failure_files.parquet'\n\n# These file names detail the local paths where we store the data results.\nMACH_LOCAL_DIRECT = 'dataingestion_mach_result.parquet'\nERROR_LOCAL_DIRECT = 'dataingestion_err_result.parquet'\nMAINT_LOCAL_DIRECT = 'dataingestion_maint_result.parquet'\nTELEMETRY_LOCAL_DIRECT = 'dataingestion_tel_result.parquet'\nFAILURES_LOCAL_DIRECT = 'dataingestion_fail_result.parquet'\n\n# This is the final data file.\nFEATURES_LOCAL_DIRECT = 'feeder_features.parquet'\n\n\n# ### Machines data set\n# \n# Now, we load the machines data set from your Azure blob.\n\n\n# create a local path  to store the data.\nif not os.path.exists(MACH_LOCAL_DIRECT):\n    os.makedirs(MACH_LOCAL_DIRECT)\n    print('DONE creating a local directory!')\n\n# Connect to blob storage container\n\n# Read in the data\nprint(MACH_LOCAL_DIRECT)\nmachines = spark.read.parquet(MACH_DATA)\n\nprint(machines.count())\nmachines.limit(5).toPandas().head()\n\n\n# ### Errors data set\n# \n# Load the errors data set from your Azure blob.\nif not os.path.exists(ERROR_LOCAL_DIRECT):\n    os.makedirs(ERROR_LOCAL_DIRECT)\n    print('DONE creating a local directory!')\n\n\n# Read in the data\nerrors = spark.read.parquet(ERROR_DATA)\n\nprint(errors.count())\nerrors.printSchema()\nerrors.limit(5).toPandas().head()\n\n\n# ### Maintenance data set\n# \n# Load the maintenance data set from your Azure blob.\n\n# create a local path  to store the data.\nif not os.path.exists(MAINT_LOCAL_DIRECT):\n    os.makedirs(MAINT_LOCAL_DIRECT)\n    print('DONE creating a local directory!')\n\n# Connect to blob storage container\n\n# Read in the data\nmaint = spark.read.parquet(MAINT_DATA)\n\nprint(maint.count())\nmaint.limit(5).toPandas().head()\n\n\n# ### Telemetry\n# \n# Load the telemetry data set from your Azure blob.\n\n\n# create a local path  to store the data.\nif not os.path.exists(TELEMETRY_LOCAL_DIRECT):\n    os.makedirs(TELEMETRY_LOCAL_DIRECT)\n    print('DONE creating a local directory!')\n\n# Connect to blob storage container\n\n# Read in the data\ntelemetry = spark.read.parquet(TELEMETRY_DATA)\n\nprint(telemetry.count())\ntelemetry.limit(5).toPandas().head()\n\n\n# ### Failures data set\n# \n# Load the failures data set from your Azure blob.\n\n\n\n# rolling mean and standard deviation\n# Temporary storage for rolling means\ntel_mean = telemetry\n\n# Which features are we interested in telemetry data set\nrolling_features = ['volt','rotate', 'pressure', 'vibration']\n      \n# n hours = n * 3600 seconds  \ntime_val = 12 * 3600\n\n# Choose the time_val hour timestamps to align the data\n# dt_truncated looks at the column named \"datetime\" in the current data set.\n# remember that Spark is lazy... this doesn't execute until it is in a withColumn statement.\ndt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val).cast(\"timestamp\"))\n# We choose windows for our rolling windows 12hrs, 24 hrs and 36 hrs\nlags = [12, 24, 36]\n\n# align the data\nfor lag_n in lags:\n    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n    for col_name in rolling_features:\n        tel_mean = tel_mean.withColumn(col_name+'_rollingmean_'+str(lag_n), \n                                       F.avg(col(col_name)).over(wSpec))\n        tel_mean = tel_mean.withColumn(col_name+'_rollingstd_'+str(lag_n), \n                                       F.stddev(col(col_name)).over(wSpec))\n\n# Calculate lag values...\ntelemetry_feat = (tel_mean.withColumn(\"dt_truncated\", dt_truncated)\n                  .drop('volt', 'rotate', 'pressure', 'vibration')\n                  .fillna(0)\n                  .groupBy(\"machineID\",\"dt_truncated\")\n                  .agg(F.mean('volt_rollingmean_12').alias('volt_rollingmean_12'),\n                       F.mean('rotate_rollingmean_12').alias('rotate_rollingmean_12'), \n                       F.mean('pressure_rollingmean_12').alias('pressure_rollingmean_12'), \n                       F.mean('vibration_rollingmean_12').alias('vibration_rollingmean_12'), \n                       F.mean('volt_rollingmean_24').alias('volt_rollingmean_24'),\n                       F.mean('rotate_rollingmean_24').alias('rotate_rollingmean_24'), \n                       F.mean('pressure_rollingmean_24').alias('pressure_rollingmean_24'), \n                       F.mean('vibration_rollingmean_24').alias('vibration_rollingmean_24'),\n                       F.mean('volt_rollingmean_36').alias('volt_rollingmean_36'),\n                       F.mean('vibration_rollingmean_36').alias('vibration_rollingmean_36'),\n                       F.mean('rotate_rollingmean_36').alias('rotate_rollingmean_36'), \n                       F.mean('pressure_rollingmean_36').alias('pressure_rollingmean_36'), \n                       F.stddev('volt_rollingstd_12').alias('volt_rollingstd_12'),\n                       F.stddev('rotate_rollingstd_12').alias('rotate_rollingstd_12'), \n                       F.stddev('pressure_rollingstd_12').alias('pressure_rollingstd_12'), \n                       F.stddev('vibration_rollingstd_12').alias('vibration_rollingstd_12'), \n                       F.stddev('volt_rollingstd_24').alias('volt_rollingstd_24'),\n                       F.stddev('rotate_rollingstd_24').alias('rotate_rollingstd_24'), \n                       F.stddev('pressure_rollingstd_24').alias('pressure_rollingstd_24'), \n                       F.stddev('vibration_rollingstd_24').alias('vibration_rollingstd_24'),\n                       F.stddev('volt_rollingstd_36').alias('volt_rollingstd_36'),\n                       F.stddev('rotate_rollingstd_36').alias('rotate_rollingstd_36'), \n                       F.stddev('pressure_rollingstd_36').alias('pressure_rollingstd_36'), \n                       F.stddev('vibration_rollingstd_36').alias('vibration_rollingstd_36'), ))\n\nprint(telemetry_feat.count())\ntelemetry_feat.where((col(\"machineID\") == 1)).limit(10).toPandas().head(10)\n\n\n# ## Errors features\n# \n# Like telemetry data, errors come with timestamps. An important difference is that the error IDs are categorical values and should not be averaged over time intervals like the telemetry measurements. Instead, we count the number of errors of each type within a lag window. \n# \n# Again, we align the error counts data by tumbling over the 12 hour window using a join with telemetry data. \n\n# create a column for each errorID \nerror_ind = (errors.groupBy(\"machineID\",\"datetime\",\"errorID\").pivot('errorID')\n             .agg(F.count('machineID').alias('dummy')).drop('errorID').fillna(0)\n             .groupBy(\"machineID\",\"datetime\")\n             .agg(F.sum('error1').alias('error1sum'), \n                  F.sum('error2').alias('error2sum'), \n                  F.sum('error3').alias('error3sum'), \n                  F.sum('error4').alias('error4sum'), \n                  F.sum('error5').alias('error5sum')))\n\n# join the telemetry data with errors\nerror_count = (telemetry.join(error_ind, \n                              ((telemetry['machineID'] == error_ind['machineID']) \n                               & (telemetry['datetime'] == error_ind['datetime'])), \"left\")\n               .drop('volt', 'rotate', 'pressure', 'vibration')\n               .drop(error_ind.machineID).drop(error_ind.datetime)\n               .fillna(0))\n\nerror_features = ['error1sum','error2sum', 'error3sum', 'error4sum', 'error5sum']\n\nwSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-24, 0)\nfor col_name in error_features:\n    # We're only interested in the erros in the previous 24 hours.\n    error_count = error_count.withColumn(col_name+'_rollingmean_24', \n                                         F.avg(col(col_name)).over(wSpec))\n\nerror_feat = (error_count.withColumn(\"dt_truncated\", dt_truncated)\n              .drop('error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum').fillna(0)\n              .groupBy(\"machineID\",\"dt_truncated\")\n              .agg(F.mean('error1sum_rollingmean_24').alias('error1sum_rollingmean_24'), \n                   F.mean('error2sum_rollingmean_24').alias('error2sum_rollingmean_24'), \n                   F.mean('error3sum_rollingmean_24').alias('error3sum_rollingmean_24'), \n                   F.mean('error4sum_rollingmean_24').alias('error4sum_rollingmean_24'), \n                   F.mean('error5sum_rollingmean_24').alias('error5sum_rollingmean_24')))\n\nprint(error_feat.count())\nerror_feat.limit(10).toPandas().head(10)\n\n\n\n# create a column for each component replacement\nmaint_replace = (maint.groupBy(\"machineID\",\"datetime\",\"comp\").pivot('comp')\n                 .agg(F.count('machineID').alias('dummy')).fillna(0)\n                 .groupBy(\"machineID\",\"datetime\")\n                 .agg(F.sum('comp1').alias('comp1sum'), \n                      F.sum('comp2').alias('comp2sum'), \n                      F.sum('comp3').alias('comp3sum'),\n                      F.sum('comp4').alias('comp4sum')))\n\nmaint_replace = maint_replace.withColumnRenamed('datetime','datetime_maint')\n\nprint(maint_replace.count())\nmaint_replace.limit(10).toPandas().head(10)\n\n\n# Replacement features are then created by tracking the number of days between each component replacement. We'll repeat these calculations for each of the four components and join them together into a maintenance feature table.\n# \n# First component number 1 (`comp1`):\n\n\n# We want to align the component information on telemetry features timestamps.\ntelemetry_times = (telemetry_feat.select(telemetry_feat.machineID, telemetry_feat.dt_truncated)\n                   .withColumnRenamed('dt_truncated','datetime_tel'))\n\n# Grab component 1 records\nmaint_comp1 = (maint_replace.where(col(\"comp1sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp2sum', 'comp3sum', 'comp4sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp1 = (telemetry_times.join(maint_comp1, \n                                        ((telemetry_times ['machineID']== maint_comp1['machineID']) \n                                         & (telemetry_times ['datetime_tel'] > maint_comp1['datetime_maint']) \n                                         & ( maint_comp1['comp1sum'] == '1')))\n                   .drop(maint_comp1.machineID))\n\n# Calculate the number of days between replacements\ncomp1 = (maint_tel_comp1.withColumn(\"sincelastcomp1\", \n                                    datediff(maint_tel_comp1.datetime_tel, maint_tel_comp1.datetime_maint))\n         .drop(maint_tel_comp1.datetime_maint).drop(maint_tel_comp1.comp1sum))\n\nprint(comp1.count())\ncomp1.filter(comp1.machineID == '625').orderBy(comp1.datetime_tel).limit(20).toPandas().head(20)\n\n\n# Then component 2 (`comp2`):\n# Grab component 2 records\nmaint_comp2 = (maint_replace.where(col(\"comp2sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp1sum', 'comp3sum', 'comp4sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp2 = (telemetry_times.join(maint_comp2, \n                                        ((telemetry_times ['machineID']== maint_comp2['machineID']) \n                                         & (telemetry_times ['datetime_tel'] > maint_comp2['datetime_maint']) \n                                         & ( maint_comp2['comp2sum'] == '1')))\n                   .drop(maint_comp2.machineID))\n\n# Calculate the number of days between replacements\ncomp2 = (maint_tel_comp2.withColumn(\"sincelastcomp2\", \n                                    datediff(maint_tel_comp2.datetime_tel, maint_tel_comp2.datetime_maint))\n         .drop(maint_tel_comp2.datetime_maint).drop(maint_tel_comp2.comp2sum))\n\nprint(comp2.count())\ncomp2.filter(comp2.machineID == '625').orderBy(comp2.datetime_tel).limit(5).toPandas().head(5)\n\n\n# Then component 3 (`comp3`):\n# Grab component 3 records\nmaint_comp3 = (maint_replace.where(col(\"comp3sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp1sum', 'comp2sum', 'comp4sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp3 = (telemetry_times.join(maint_comp3, ((telemetry_times ['machineID']==maint_comp3['machineID']) \n                                                      & (telemetry_times ['datetime_tel'] > maint_comp3['datetime_maint']) \n                                                      & ( maint_comp3['comp3sum'] == '1')))\n                   .drop(maint_comp3.machineID))\n\n# Calculate the number of days between replacements\ncomp3 = (maint_tel_comp3.withColumn(\"sincelastcomp3\", \n                                    datediff(maint_tel_comp3.datetime_tel, maint_tel_comp3.datetime_maint))\n         .drop(maint_tel_comp3.datetime_maint).drop(maint_tel_comp3.comp3sum))\n\n\nprint(comp3.count())\ncomp3.filter(comp3.machineID == '625').orderBy(comp3.datetime_tel).limit(5).toPandas().head(5)\n\n\n# and component 4 (`comp4`):\n# Grab component 4 records\nmaint_comp4 = (maint_replace.where(col(\"comp4sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n               .drop('comp1sum', 'comp2sum', 'comp3sum'))\n\n# Within each machine, get the last replacement date for each timepoint\nmaint_tel_comp4 = telemetry_times.join(maint_comp4, ((telemetry_times['machineID']==maint_comp4['machineID']) \n                                                     & (telemetry_times['datetime_tel'] > maint_comp4['datetime_maint']) \n                                                     & (maint_comp4['comp4sum'] == '1'))).drop(maint_comp4.machineID)\n\n# Calculate the number of days between replacements\ncomp4 = (maint_tel_comp4.withColumn(\"sincelastcomp4\", \n                                    datediff(maint_tel_comp4.datetime_tel, maint_tel_comp4.datetime_maint))\n         .drop(maint_tel_comp4.datetime_maint).drop(maint_tel_comp4.comp4sum))\n\nprint(comp4.count())\ncomp4.filter(comp4.machineID == '625').orderBy(comp4.datetime_tel).limit(5).toPandas().head(5)\n\n\n# Now, we join the four component replacement tables together. Once joined, align the data by tumbling the average across 12 hour observation windows.\n\n# Join component 3 and 4\ncomp3_4 = (comp3.join(comp4, ((comp3['machineID'] == comp4['machineID']) \n                              & (comp3['datetime_tel'] == comp4['datetime_tel'])), \"left\")\n           .drop(comp4.machineID).drop(comp4.datetime_tel))\n\n# Join component 2 to 3 and 4\ncomp2_3_4 = (comp2.join(comp3_4, ((comp2['machineID'] == comp3_4['machineID']) \n                                  & (comp2['datetime_tel'] == comp3_4['datetime_tel'])), \"left\")\n             .drop(comp3_4.machineID).drop(comp3_4.datetime_tel))\n\n# Join component 1 to 2, 3 and 4\ncomps_feat = (comp1.join(comp2_3_4, ((comp1['machineID'] == comp2_3_4['machineID']) \n                                      & (comp1['datetime_tel'] == comp2_3_4['datetime_tel'])), \"left\")\n               .drop(comp2_3_4.machineID).drop(comp2_3_4.datetime_tel)\n               .groupBy(\"machineID\", \"datetime_tel\")\n               .agg(F.max('sincelastcomp1').alias('sincelastcomp1'), \n                    F.max('sincelastcomp2').alias('sincelastcomp2'), \n                    F.max('sincelastcomp3').alias('sincelastcomp3'), \n                    F.max('sincelastcomp4').alias('sincelastcomp4'))\n               .fillna(0))\n\n# Choose the time_val hour timestamps to align the data\ndt_truncated = ((round(unix_timestamp(col(\"datetime_tel\")) / time_val) * time_val).cast(\"timestamp\"))\n\n# Collect data\nmaint_feat = (comps_feat.withColumn(\"dt_truncated\", dt_truncated)\n              .groupBy(\"machineID\",\"dt_truncated\")\n              .agg(F.mean('sincelastcomp1').alias('comp1sum'), \n                   F.mean('sincelastcomp2').alias('comp2sum'), \n                   F.mean('sincelastcomp3').alias('comp3sum'), \n                   F.mean('sincelastcomp4').alias('comp4sum')))\n\nprint(maint_feat.count())\nmaint_feat.limit(10).toPandas().head(10)\n\n\n# ## Machine features\n# \n# The machine features capture specifics of the individuals. These can be used without further modification since it include descriptive information about the type of each machine and its age (number of years in service). If the age information had been recorded as a \"first use date\" for each machine, a transformation would have been necessary to turn those into a numeric values indicating the years in service.\n# \n# We do need to create a set of dummy features, a set of boolean variables, to indicate the model of the machine. This can either be done manually, or using a _one-hot encoding_ step. We use the -hot encoding for demonstration purposes. \n\n\n# one hot encoding of the variable model, basically creates a set of dummy boolean variables\ncatVarNames = ['model']  \nsIndexers = [StringIndexer(inputCol=x, outputCol=x + '_indexed') for x in catVarNames]\nmachines_cat = Pipeline(stages=sIndexers).fit(machines).transform(machines)\n\n# one-hot encode\nohEncoders = [OneHotEncoder(inputCol=x + '_indexed', outputCol=x + '_encoded')\n              for x in catVarNames]\n\nohPipelineModel = Pipeline(stages=ohEncoders).fit(machines_cat)\nmachines_cat = ohPipelineModel.transform(machines_cat)\n\ndrop_list = [col_n for col_n in machines_cat.columns if 'indexed' in col_n]\n\nmachines_feat = machines_cat.select([column for column in machines_cat.columns if column not in drop_list])\n\nprint(machines_feat.count())\nmachines_feat.limit(10).toPandas().head(10)\n\n\n# ## Merging feature data\n# \n# Next, we merge the telemetry, maintenance, machine and error feature data sets into a large feature data set. Since most of the data has already been aligned on the 12 hour observation period, we can merge with a simple join strategy.\n\n\n# join error features with component maintenance features\nerror_maint = (error_feat.join(maint_feat, \n                               ((error_feat['machineID'] == maint_feat['machineID']) \n                                & (error_feat['dt_truncated'] == maint_feat['dt_truncated'])), \"left\")\n               .drop(maint_feat.machineID).drop(maint_feat.dt_truncated))\n\n# now join that with machines features\nerror_maint_feat = (error_maint.join(machines_feat, \n                                     ((error_maint['machineID'] == machines_feat['machineID'])), \"left\")\n                    .drop(machines_feat.machineID))\n\n# Clean up some unecessary columns\nerror_maint_feat = error_maint_feat.select([c for c in error_maint_feat.columns if c not in \n                                            {'error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum'}])\n\n# join telemetry with error/maint/machine features to create final feature matrix\nfinal_feat = (telemetry_feat.join(error_maint_feat, \n                                  ((telemetry_feat['machineID'] == error_maint_feat['machineID']) \n                                   & (telemetry_feat['dt_truncated'] == error_maint_feat['dt_truncated'])), \"left\")\n              .drop(error_maint_feat.machineID).drop(error_maint_feat.dt_truncated))\n\nprint(final_feat.count())\nfinal_feat.filter(final_feat.machineID == '625').orderBy(final_feat.dt_truncated).limit(10).toPandas().head(10)\n\n\ndt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val).cast(\"timestamp\"))\n\n\n\nprint [ i for i in final_feat ] \nfinal_feat.drop()\nfinal_feat.write.mode('overwrite').parquet(FEATURES_LOCAL_DIRECT)\n#final_feat.write.format(\"org.apache.spark.sql.insightedge\").mode(\"overwrite\").save(FEATURES_LOCAL_DIRECT)\n\n# Delete the old data.\n\n# upload the entire folder into blob storage\n\nprint(\"Feature engineering final dataset files saved!\")\n\n# Time the notebook execution. \n# This will only make sense if you \"Run All\" cells\ntoc = time.time()\nprint(\"Full run took %.2f minutes\" % ((toc - tic)/60))","user":"anonymous","dateUpdated":"2019-06-20T20:20:34+0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560941451335_-1309080552","id":"20190619-135051_308748241","dateCreated":"2019-06-19T13:50:51+0300","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:260"}],"name":"Feeder","id":"2EE79AGSD","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}